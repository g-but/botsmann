/**
 * Demo Document Chat API
 *
 * POST /api/demo/document-chat - Chat with document content (no auth required)
 *
 * This endpoint allows public demo of document chat functionality.
 * Document content is sent with each request (stored client-side).
 */

/* eslint-disable no-console */

import { type NextRequest } from 'next/server';
import { generateLLMResponse } from '@/lib/llm-client';
import { jsonSuccess, jsonError, HTTP_STATUS } from '@/lib/api';
import { rateLimit } from '@/lib/rate-limit';
import { getClientIp } from '@/lib/request';

// Extend function timeout for model loading (Vercel)
export const maxDuration = 30;

// Maximum document content size (characters) to prevent abuse
const MAX_DOCUMENT_SIZE = 50000; // ~50KB of text
const MAX_CONTEXT_CHARS = 8000; // Limit context sent to LLM

export async function POST(request: NextRequest) {
  const startTime = Date.now();
  console.log('[Demo Document Chat] Starting request');

  try {
    // Rate limit per IP (stricter since no auth)
    const limiter = rateLimit({ limit: 15, interval: 60 * 1000, uniqueTokenPerInterval: 1000 });
    const ip = getClientIp(request);
    const { isRateLimited } = await limiter.check(`demo-doc-chat:${ip}`);
    if (isRateLimited) {
      return jsonError('Too many requests. Please wait a moment.', 'RATE_LIMIT', HTTP_STATUS.RATE_LIMIT);
    }

    const body = await request.json();
    const { message, documents } = body;

    if (!message || typeof message !== 'string') {
      return jsonError('Message required', 'VALIDATION_ERROR', HTTP_STATUS.BAD_REQUEST);
    }

    if (!documents || !Array.isArray(documents) || documents.length === 0) {
      return jsonError('At least one document required', 'VALIDATION_ERROR', HTTP_STATUS.BAD_REQUEST);
    }

    // Validate and truncate document content
    let totalChars = 0;
    const processedDocs: Array<{ name: string; content: string }> = [];

    for (const doc of documents) {
      if (!doc.name || !doc.content) continue;

      let content = String(doc.content);

      // Truncate individual documents if too large
      if (content.length > MAX_DOCUMENT_SIZE) {
        content = content.substring(0, MAX_DOCUMENT_SIZE) + '\n\n[Document truncated...]';
      }

      // Check total context size
      if (totalChars + content.length > MAX_CONTEXT_CHARS) {
        const remaining = MAX_CONTEXT_CHARS - totalChars;
        if (remaining > 500) {
          content = content.substring(0, remaining) + '\n\n[Content truncated for context limit...]';
          processedDocs.push({ name: doc.name, content });
        }
        break;
      }

      processedDocs.push({ name: doc.name, content });
      totalChars += content.length;
    }

    if (processedDocs.length === 0) {
      return jsonError('No valid document content provided', 'VALIDATION_ERROR', HTTP_STATUS.BAD_REQUEST);
    }

    // Build context from documents
    const contextParts = processedDocs.map((doc, index) =>
      `[Document ${index + 1}: ${doc.name}]\n${doc.content}`
    );
    const context = contextParts.join('\n\n---\n\n');

    const systemPrompt = `You are a helpful assistant that answers questions based on the provided documents.

Guidelines:
- Answer questions using ONLY the information from the provided documents
- If the answer is not in the documents, say so clearly
- Quote relevant passages when helpful
- Be concise but thorough
- If asked about something not in the documents, explain what the documents do contain

Documents have been provided below. Use them to answer the user's question.`;

    console.log('[Demo Document Chat] Calling LLM, context chars:', totalChars);
    const llmStartTime = Date.now();

    try {
      const llmResponse = await generateLLMResponse(
        [
          { role: 'system', content: systemPrompt },
          {
            role: 'user',
            content: `Documents:\n\n${context}\n\n---\n\nQuestion: ${message}`,
          },
        ],
        {
          provider: 'groq', // Use Groq (fast and free tier)
          temperature: 0.3, // Lower temperature for more factual responses
          maxTokens: 1024,
        },
      );

      console.log('[Demo Document Chat] LLM response in', Date.now() - llmStartTime, 'ms');
      console.log('[Demo Document Chat] Total time:', Date.now() - startTime, 'ms');

      return jsonSuccess({
        response: llmResponse.content,
        sources: processedDocs.map(doc => ({
          document_name: doc.name,
          preview: doc.content.substring(0, 150) + (doc.content.length > 150 ? '...' : ''),
        })),
        provider: llmResponse.provider,
        model: llmResponse.model,
      });
    } catch (llmError) {
      console.error('[Demo Document Chat] LLM error:', llmError);

      // Return document excerpts as fallback
      if (processedDocs.length > 0) {
        const excerpts = processedDocs.map(doc =>
          `**${doc.name}:**\n${doc.content.substring(0, 500)}${doc.content.length > 500 ? '...' : ''}`
        ).join('\n\n');

        return jsonSuccess({
          response: `I'm having trouble generating a response, but here's what I found in your documents:\n\n${excerpts}`,
          sources: processedDocs.map(doc => ({
            document_name: doc.name,
            preview: doc.content.substring(0, 150),
          })),
          provider: 'fallback',
          model: 'none',
        });
      }

      return jsonError(
        'AI service temporarily unavailable. Please try again.',
        'SERVICE_UNAVAILABLE',
        HTTP_STATUS.SERVICE_UNAVAILABLE,
      );
    }
  } catch (error) {
    console.error('[Demo Document Chat] Unhandled error:', error);
    return jsonError('Internal server error', 'INTERNAL_ERROR', HTTP_STATUS.INTERNAL_ERROR);
  }
}
